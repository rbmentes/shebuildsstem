<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />

  <title>Speech AI Starter Series ‚Äî Part 1</title>

  <!-- Main Site CSS -->
  <link rel="stylesheet" href="style.css" />
  <link rel="icon" href="favicon.jpg" />

  <!-- Font -->
  <link
    href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;600&display=swap"
    rel="stylesheet"
  />
</head>

<body>

  <!-- Blog Container -->
  <section class="post-page">

    <div class="post-card">

      <a class="back-btn" href="blog.html">‚Üê Back to Blog</a>

      <h1>üéß Speech AI Starter Series ‚Äî Part 1</h1>
      <p class="subtitle">Understanding Audio Features & MFCC</p>

      <p>
        When we record audio, what we actually capture is not ‚Äúsound‚Äù in the way we feel it ‚Äî
        we capture a long sequence of numbers.
      </p>

      <p>
        These numbers represent <b>air pressure changes over time</b>.
        This is called a waveform. Speech, music, noise‚Ä¶ everything becomes a signal.
      </p>

      <hr />

      <h2>üåä Why Can‚Äôt We Use Raw Audio Directly?</h2>

      <p>
        At first, you might think:
        <b>‚ÄúWhy not just give the waveform to a machine learning model?‚Äù</b>
      </p>

      <p>The problem is that raw audio is:</p>

      <ul>
        <li>Extremely long (thousands of samples per second)</li>
        <li>Very noisy and detailed</li>
        <li>Hard for classical ML models to understand</li>
      </ul>

      <p>
        So instead of using raw samples, we transform audio into something more meaningful:
        <b>features</b>.
      </p>

      <hr />

      <h2>‚ú® Feature Extraction in Speech AI</h2>

      <p>
        Feature extraction means converting audio into a compact representation that keeps
        the important information.
      </p>

      <p>For speech tasks, we want to capture things like:</p>

      <ul>
        <li>Pitch and tone</li>
        <li>Pronunciation patterns</li>
        <li>Frequency structure of speech</li>
        <li>Language-specific characteristics</li>
      </ul>

      <p>
        One of the most famous feature extraction methods is:
        <b>MFCC</b>.
      </p>

      <hr />

      <h2>üí° What Is MFCC?</h2>

      <p><b>MFCC = Mel-Frequency Cepstral Coefficients</b></p>

      <p>
        MFCC is a way to represent speech by focusing on how humans actually hear sound.
      </p>

      <p>
        Instead of keeping every raw detail, MFCC captures the <b>shape of the spectrum</b>
        in a compact form.
      </p>

      <hr />

      <h2>‚öôÔ∏è How MFCC Is Computed (Simplified)</h2>

      <ol>
        <li>Split audio into small frames</li>
        <li>Apply Fourier Transform</li>
        <li>Map frequencies onto the Mel scale</li>
        <li>Take logarithm</li>
        <li>Apply DCT to compress into coefficients</li>
      </ol>

      <hr />

      <h2>‚ö†Ô∏è Limitations of MFCC</h2>

      <ul>
        <li>Handcrafted feature engineering</li>
        <li>May miss complex patterns</li>
        <li>Deep learning models outperform it today</li>
      </ul>

      <hr />

      <h2>üå∏ My Experience</h2>

      <p>
        In my Language Identification project, MFCC was the first method I tried.
        It helped me understand the basics of speech processing.
      </p>

      <p>
        In Part 2, I will explain how CNNs improved the results.
      </p>

      <h2>‚û°Ô∏è Coming Next</h2>
      <p><b>Part 2: CNNs for Speech Recognition üéß‚ö°</b></p>

    </div>
  </section>

</body>
</html>

